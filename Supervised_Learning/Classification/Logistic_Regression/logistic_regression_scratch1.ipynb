{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize Parameters\n",
    "* Learning Rate\n",
    "* Batch size\n",
    "* Number of iteration\n",
    "* weight\n",
    "* bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression: \n",
    "    def __init__(self, learning_rate = 0.001, n_iteration = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iteration = n_iteration\n",
    "        self.weights = None\n",
    "        self.bias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sigmoid Function\n",
    "Takes any real value as input(x) and outputs values in the range of 0 to 1. The larget the input, the closer the output value will be to 1. The smaller the input, the closer the output will be to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x):\n",
    "     return 1 / (1 + np.exp(-x))\n",
    "LogisticRegression.sigmoid = sigmoid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Used for models where we have to predict the probability as an output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binary Cross Entropy Loss\n",
    " Models how well an algorithm models the dataset\n",
    " * The binary cross entropy loss function will output a higher number if the predictions are really off\n",
    "    * Outputs a lower number if the predictions are good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_cross_entropy_loss(self, y_true, y_pred):\n",
    "    epsilon = 1e-9\n",
    "    y1 = y_true * np.log(y_pred + epsilon)\n",
    "    y2 = (1- y_true) * np.log(1 - y_pred + epsilon)\n",
    "    return -np.mean(y1 + y2)\n",
    "LogisticRegression.binary_cross_entropy_loss = binary_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feed_forward(self, X):\n",
    "    z = np.dot(X, self.weights) + self.bias\n",
    "    A = sigmoid(z)\n",
    "    return A\n",
    "\n",
    "LogisticRegression.feed_forward = feed_forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fit data using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y): \n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    #init parameters\n",
    "    self.weights = np.zeros(n_features)\n",
    "    self.bias = 0\n",
    "\n",
    "    # gradient descent\n",
    "    for _ in range(self.n_iteration):\n",
    "        A = self.feed_forward(X)\n",
    "        dz = A - y #derivative of sigmoid and bceloss\n",
    "\n",
    "        # Compute gradients\n",
    "        dw = (1 / n_samples) * np.dot(X.T, dz)\n",
    "        db = (1 / n_samples) * np.sum(dz)\n",
    "        # Update parameters\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "\n",
    "LogisticRegression.fit = fit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    threshold = .5\n",
    "    y_hat = np.dot(X, self.weights) + self.bias\n",
    "    y_predicted = sigmoid(y_hat)\n",
    "    y_predicted_cls = [1 if i > threshold else 0 for i in y_predicted]\n",
    "\n",
    "    return np.array(y_predicted_cls)\n",
    "\n",
    "LogisticRegression.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_actual, y_predicted):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    epsilon = 1e-9\n",
    "    for i in range(len(y_actual)):\n",
    "        if y_actual[i] > 0:\n",
    "            if y_actual[i] == y_predicted[i]:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "        if y_actual[i] < 1:\n",
    "            if y_actual[i] == y_predicted[i]:\n",
    "                tn = tn + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "\n",
    "    cm = [[tn, fp], [fn, tp]]\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn+epsilon)\n",
    "    sens = tp/(tp+fn+epsilon)\n",
    "    prec = tp/(tp+fp+epsilon)\n",
    "    f_score = (2*prec*sens)/(prec+sens+epsilon)\n",
    "    return cm,accuracy,sens,prec,f_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output the Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.930\n",
      "Confusion Matrix: [[39  6]\n",
      " [ 2 67]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=1234)\n",
    "\n",
    "regressor = LogisticRegression(learning_rate=0.0001, n_iteration=1000)\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "cm, accuracy, sens, precision, f_score = confusion_matrix(np.asarray(y_test), np.asarray(predictions))\n",
    "print(\"Test accuracy: {0:.3f}\".format(accuracy))\n",
    "print(\"Confusion Matrix:\",np.array(cm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
